import logging
import gensim
from gensim.models import word2vec, KeyedVectors
import pandas as pd
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import adjustText
import numpy as np
from nltk.tokenize import MWETokenizer
file1 = open('1960-2020.txt', 'r', encoding='utf-8') # 打开要去掉空行的文件
file2 = open('new.txt', 'w', encoding='utf-8') # 生成没有空行的文件

for line in file1.readlines():
    if line == '\n':
       line = line.strip("\n")
    file2.write(line)
print('输出成功....')
file1.close()
file2.close()

sentences = word2vec.Text8Corpus('new.txt')

tokenizer = MWETokenizer([('solar', 'cell')], separator = '_') #自定义词典，将‘solar’和‘cell’结合

newsvec = [tokenizer.tokenize(nltk.word_tokenize(str(sentences)))] #用nltk自定义词典将数据库分词并保留特定词组

model = word2vec.Word2Vec(newsvec, min_count=2, vector_size=100, window=5,sg=1,sample=1e-4) #训练word2vec模型,设置超参数

print("计算某个词的相关词列表")
word = 'solar_cell'
result2 = model.wv.most_similar(word, topn=3000)  # 3000个最相关的
print("和" + word + "最相关的词有：")
for item in result2:
     print(item[0])
print("\n================================")

model.wv.save_word2vec_format('w2vModel.bin')#保存为模型类文件

model = KeyedVectors.load_word2vec_format('w2vModel.bin') #以Word2vec文件形式打开刚刚保存的模型类文件

word2 = model.similarity('solar_cell ','CH3NH3PbBr3' ) #计算任意两个词向量之间的余弦相似度

print(word2)

X = np.array(model.wv['Japan','Egypt','US']) #输入需要可视化的词向量

pca = PCA(n_components=2) #选择需要降成的维度

Y = pca.fit_transform(X) #将32维词向量降维成2维

txt = ['Japan','Egypt','US'] #图中每个点的标注

x = Y[:,0] #图中每个点的横坐标

y = Y[:,1] #图中每个点的纵坐标

plt.scatter(x, y)
for i in range(len(x)):
    plt.annotate(txt[i], xy = (x[i], y[i]), xytext = (x[i]+0.002, y[i]+0.002)) #使用matplotlib在图中画出每个点

plt.show() #展示图像'''

